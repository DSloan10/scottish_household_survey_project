---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(janitor)

green_blue_comp_data <- read_csv("data/complete_dist_blue_green.csv") %>% clean_names()
neighbourhood_comp_data <- read_csv("data/complete_neighbourhood_rating.csv") %>% clean_names()
community_comp_data <- read_csv("data/complete_community_belonging.csv") %>% clean_names()
```

I really want to work out if we can get all these bound together in one big dataset.

```{r}
green_blue_comp_data %>% 
  glimpse()

neighbourhood_comp_data %>% 
  glimpse()

community_comp_data %>% 
  glimpse()
```

```{r}
codes_gb <-
green_blue_comp_data %>% 
  distinct(feature_code)

codes_gb

green_blue_comp_data %>% 
  distinct(units)
```

```{r}
codes_neigh <-
neighbourhood_comp_data %>% 
  distinct(feature_code)

codes_neigh

neighbourhood_comp_data %>% 
  distinct(units)
```

```{r}
codes_community <-
community_comp_data %>% 
  distinct(feature_code)

community_comp_data %>% 
  distinct(units)
```

Cool, so all three datasets seem to have the same amount of variables, altough there will be a slight difference in the measure of each. The thing I want to get checking is that feature codes refer to regions or, more likely, council areas. I also just want to double check that these are all the same in each dataset. I'm pretty sure we actually have the codes down for this in the previous report which would be super handy. 

```{r}
all_equal(codes_community, codes_gb, codes_neigh)
```

Sweet, it looks like all the codes match which I'm hoping means we can just do a join on all these. Not sure whether to change the codes now or after. Think I might just do now so the join is a bit clearer for me, although I'm guessing it would be best practice to do it after.

My mistake earlier, just realised those were NHS codes not council ones. 

```{r}
local_authority_codes <- read_csv("data/Local_Authority_Districts_Codes.csv") %>% clean_names()
```

```{r}
local_authority_codes %>%
  distinct(lad19cd)
```


```{r}
scot_local_auth_codes <-
local_authority_codes %>% 
  filter(str_detect(lad19cd, "S12")) %>% 
  select(lad19cd, lad19nm) %>% 
  rename(feature_code = lad19cd,
         local_authority = lad19nm)
```

```{r}
#Adding a local authority column to each of the datasets
gb_with_las_data <-
full_join(green_blue_comp_data, scot_local_auth_codes, "feature_code")

gb_with_las_data <-
gb_with_las_data %>% 
  mutate(local_authority = if_else(feature_code == "S92000003", "Scotland", local_authority))

gb_with_las_data
```

```{r}
neigh_with_las_data <-
full_join(neighbourhood_comp_data, scot_local_auth_codes, "feature_code")

neigh_with_las_data <-
neigh_with_las_data %>% 
  mutate(local_authority = if_else(feature_code == "S92000003", "Scotland", local_authority))

neigh_with_las_data
```

```{r}
commun_with_las_data <-
full_join(community_comp_data, scot_local_auth_codes, "feature_code")

commun_with_las_data <-
commun_with_las_data %>% 
  mutate(local_authority = if_else(feature_code == "S92000003", "Scotland", local_authority))

commun_with_las_data
```

I think that now would be a good time to decide on which variables we can think about ditching which will not help us with our analysis. Basically anything that we don't want to immediately look at. I think we should look at the NAs and then work out which variables to ditch in each spreadsheet.

```{r}
gb_with_las_data %>% 
summarise(across(.fns = ~sum(is.na(.x))))

neigh_with_las_data %>% 
summarise(across(.fns = ~sum(is.na(.x))))

commun_with_las_data %>%
summarise(across(.fns = ~sum(is.na(.x))))
```

Wow, we have no missing values across our data!! For three datasets, that's pretty cool. Don't quite know why it would be the case but will just double check that this was the case for the original datasets.

```{r}
green_blue_comp_data %>%
  summarise(across(.fns = ~sum(is.na(.x))))

```

Looks all good. Right let's get cracking on those variables. So here's a list for all three datasets that I feel like we can ditch (for now):

1. Feature_code - This now seems redundant considering I've made the local authority column.

2. Measuremnt - **Important** - for now I want to filter out anything but percent and then drop this variable entirely. **This is something I definitely want to come back to if I have time, but for now it will drop so many rows. Might leave this until last so it's easier to go back to.*

3. Units - from what I can see, this is redundant and we can just change the name of the "value" variable to take the fact that it is a percentage into account. 


Think that may be it for now. Going to do this in a couple of stages and then try and make the join. Hoping the joins will actually work but it's still screwing with my head to work out how they should be behaving. I can't quite seem to work out how many variables we should end up with and what they should refer to. To be honest, I think that it might be best to just draw this out on a piece of paper or in excalidraw!

```{r}
gb_filtered <-
gb_with_las_data %>%
  filter(measurement == "Percent") %>% 
  select(-c(feature_code, units, measurement))
```

```{r}
commun_filtered <-
commun_with_las_data %>%
  filter(measurement == "Percent") %>% 
  select(-c(feature_code, units, measurement))
```

```{r}
neigh_filtered <-
neigh_with_las_data %>%
  filter(measurement == "Percent") %>% 
  select(-c(feature_code, units, measurement))
```

```{r}
gb_filtered

neigh_filtered

commun_filtered
```

Ok so let's try a wee pivot wider with the values and the distances. 

```{r}
gb_filtered %>% 
  pivot_wider(names_from = distance_to_nearest_green_or_blue_space,
              values_from = value)

neigh_filtered %>% 
  pivot_wider(names_from = neighbourhood_rating,
              values_from = )
```

Right, so we seem to have hit a bit of a brick wall with the pivot for the neighbourhood and community 